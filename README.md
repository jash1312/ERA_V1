# ERA_V1


PART 1 - 


2. Backward Propagation:

In Back propagation we calculate the gradients from the loss function and using the learning rate we update our weights.

Gradient descent is where we multiply the learning rate with the loss function and subtract it from our weights

We do this activity for each epoch and idealy the loss should reduce with the number of eposchs 

We basically take partial derivative of the loss with respect to each weight using calculus and then use this to update each of those weights for each epoch and do this iteratively
